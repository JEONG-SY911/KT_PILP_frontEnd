####### ì¸êµ¬ ë°ì´í„° ë¶„ì„ìš© LangGraph Agent with RAG & Document Support #######
## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ---------------------------------------------
import pandas as pd
import numpy as np
import os
import openai
import random
import ast
from fastapi import FastAPI, HTTPException, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime
from typing import List, Dict, Any, Literal, TypedDict
import json
import logging
import warnings
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

warnings.filterwarnings("ignore", category=DeprecationWarning)

from typing import Annotated, Literal, Sequence, TypedDict, List, Dict
from langchain import hub
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.output_parsers import CommaSeparatedListOutputParser
from langgraph.graph import StateGraph, END

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
import chromadb
from chromadb.config import Settings

# Word ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from docx import Document
import fitz  # PyMuPDF
import tempfile
import shutil

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HOST = os.getenv("HOST", "0.0.0.0")
PORT = 8004  # Population LangGraph ì „ìš© í¬íŠ¸
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

# API í‚¤ í™•ì¸
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    raise ValueError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

logger.info(f"ì„œë²„ í™˜ê²½: {ENVIRONMENT}")
logger.info(f"OpenAI API í‚¤ ì„¤ì •ë¨: {'*' * 10}{OPENAI_API_KEY[-4:] if OPENAI_API_KEY else 'None'}")

app = FastAPI(title="ì¸êµ¬ ë°ì´í„° LangGraph Agent ë¶„ì„ API with RAG", version="2.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

## ---------------- RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ----------------------

class RAGSystem:
    def __init__(self):
        self.vectorstore = None
        self.embeddings = None
        self.retriever = None
        self.initialize_rag()
    
    def initialize_rag(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # í•œêµ­ì–´ ì§€ì› ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            self.vectorstore = Chroma(
                collection_name="population_knowledge",
                embedding_function=self.embeddings,
                persist_directory="./chroma_db"
            )
            
            # ê¸°ë³¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
            self.build_knowledge_base()
            
            # ê²€ìƒ‰ê¸° ì„¤ì •
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            
            logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # RAG ì—†ì´ë„ ì‘ë™í•˜ë„ë¡ ì„¤ì •
            self.retriever = None
    
    def build_knowledge_base(self):
        """ê¸°ë³¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            # ê¸°ë³¸ ì¸êµ¬ ê´€ë ¨ ì§€ì‹ ë¬¸ì„œë“¤
            knowledge_docs = [
                {
                    "content": """
                    ì„œìš¸ì‹œ ì¸êµ¬ í˜„í™© (2024ë…„ ê¸°ì¤€)
                    - ì´ ì¸êµ¬: ì•½ 950ë§Œëª…
                    - ì¸êµ¬ ë°€ë„: 15,000ëª…/kmÂ²
                    - ì£¼ìš” íŠ¹ì§•: ìˆ˜ë„ê¶Œ ì§‘ì¤‘ í˜„ìƒ, ê³ ë ¹í™” ì§„í–‰
                    - ì™¸êµ­ì¸ ë¹„ìœ¨: ì•½ 3.5%
                    """,
                    "metadata": {"source": "seoul_population", "type": "statistics"}
                },
                {
                    "content": """
                    ê°•ë‚¨êµ¬ ì§€ì—­ íŠ¹ì„±
                    - ì´ ì¸êµ¬: ì•½ 55ë§Œëª…
                    - ë©´ì : 39.5kmÂ²
                    - ì¸êµ¬ ë°€ë„: 14,000ëª…/kmÂ²
                    - ì£¼ìš” íŠ¹ì§•: IT ì—…ê³„ ì¤‘ì‹¬ì§€, ê³ ì†Œë“ì¸µ ì§‘ì¤‘, ì™¸êµ­ì¸ ë¹„ìœ¨ ë†’ìŒ
                    - ì£¼ìš” ì—…ì¢…: IT, ê¸ˆìœµ, êµìœ¡, ì˜ë£Œ
                    """,
                    "metadata": {"source": "gangnam_characteristics", "type": "regional_info"}
                },
                {
                    "content": """
                    í•œêµ­ ì¸êµ¬ ì •ì±… ë° íŠ¸ë Œë“œ
                    - ì €ì¶œì‚° ê³ ë ¹í™” ëŒ€ì‘ ì •ì±…
                    - ì™¸êµ­ì¸ ìœ ì¹˜ ì •ì±… (ê¸€ë¡œë²Œ ì¸ì¬)
                    - ìˆ˜ë„ê¶Œ ì§‘ì¤‘ ì™„í™” ì •ì±…
                    - ë””ì§€í„¸ ì „í™˜ì— ë”°ë¥¸ ì¸êµ¬ ì´ë™
                    - ì½”ë¡œë‚˜19 ì´í›„ ì›ê²©ê·¼ë¬´ í™•ì‚°
                    """,
                    "metadata": {"source": "korea_policy", "type": "policy"}
                },
                {
                    "content": """
                    ì¸êµ¬ ë¶„ì„ ì§€í‘œ í•´ì„ ê°€ì´ë“œ
                    - ì™¸êµ­ì¸ ë¹„ìœ¨ 10% ì´ìƒ: ê¸€ë¡œë²Œ ê¸°ì—… ì§‘ì¤‘ ì§€ì—­
                    - 20-30ëŒ€ ë¹„ìœ¨ 40% ì´ìƒ: ì Šì€ ì¸µ ì§‘ì¤‘ ì§€ì—­
                    - ì¸êµ¬ ë°€ë„ 20,000ëª…/kmÂ² ì´ìƒ: ê³ ë°€ë„ ë„ì‹œ ì§€ì—­
                    - ì£¼ì•¼ê°„ ì¸êµ¬ ì°¨ì´ 30% ì´ìƒ: ì—…ë¬´ ì¤‘ì‹¬ ì§€ì—­
                    """,
                    "metadata": {"source": "analysis_guide", "type": "interpretation"}
                },
                {
                    "content": """
                    ê°•ë‚¨êµ¬ í–‰ì •ë™ë³„ íŠ¹ì„±
                    - ì—­ì‚¼ë™: IT ê¸°ì—… ë°€ì§‘, ì™¸êµ­ì¸ ë§ìŒ
                    - ì‚¼ì„±ë™: ëŒ€ê¸°ì—… ë³¸ì‚¬, ê³ ê¸‰ ì£¼ê±°ì§€
                    - ì²­ë‹´ë™: ê³ ê¸‰ ìƒê¶Œ, ì£¼ê±°ì§€
                    - ì‹ ì‚¬ë™: ìƒê¶Œ, ì£¼ê±°ì§€ í˜¼ì¬
                    - ë…¼í˜„ë™: ìƒê¶Œ ì¤‘ì‹¬, ì£¼ê±°ì§€
                    """,
                    "metadata": {"source": "gangnam_dongs", "type": "regional_detail"}
                }
            ]
            
            # ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            texts = [doc["content"] for doc in knowledge_docs]
            metadatas = [doc["metadata"] for doc in knowledge_docs]
            
            self.vectorstore.add_texts(texts=texts, metadatas=metadatas)
            logger.info(f"ê¸°ë³¸ ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(knowledge_docs)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
    
    def add_custom_knowledge(self, content: str, metadata: Dict = None):
        """ì‚¬ìš©ì ì •ì˜ ì§€ì‹ ì¶”ê°€"""
        try:
            if metadata is None:
                metadata = {"source": "custom", "type": "user_defined"}
            
            self.vectorstore.add_texts(texts=[content], metadatas=[metadata])
            logger.info("ì‚¬ìš©ì ì •ì˜ ì§€ì‹ ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì •ì˜ ì§€ì‹ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    def retrieve_relevant_context(self, query: str, k: int = 3) -> List[str]:
        """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        try:
            if self.retriever is None:
                return []
            
            docs = self.retriever.get_relevant_documents(query)
            contexts = [doc.page_content for doc in docs[:k]]
            return contexts
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

## ---------------- ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ ----------------------

def extract_text_from_file(file_path: str) -> str:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDF, DOCX ì§€ì›)"""
    try:
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == ".pdf":
            doc = fitz.open(file_path)
            text = "\n".join(page.get_text() for page in doc)
            doc.close()
            return text
            
        elif ext == ".docx":
            doc = Document(file_path)
            text_parts = []
            
            # ë‹¨ë½ í…ìŠ¤íŠ¸
            for p in doc.paragraphs:
                if p.text.strip():
                    text_parts.append(p.text)
            
            # í‘œ ë°ì´í„°
            for table in doc.tables:
                for row in table.rows:
                    row_text = " | ".join(cell.text.strip() for cell in row.cells if cell.text.strip())
                    if row_text:
                        text_parts.append(row_text)
            
            return "\n".join(text_parts)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
        raise

def extract_document_metadata(file_path: str) -> Dict[str, Any]:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    try:
        metadata = {
            "file_name": os.path.basename(file_path),
            "file_size": os.path.getsize(file_path),
            "created_date": datetime.fromtimestamp(os.path.getctime(file_path)).isoformat(),
            "modified_date": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
        }
        
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == ".docx":
            try:
                doc = Document(file_path)
                # ë¬¸ì„œ ì†ì„± ì¶”ì¶œ
                if doc.core_properties.title:
                    metadata["title"] = doc.core_properties.title
                if doc.core_properties.author:
                    metadata["author"] = doc.core_properties.author
                if doc.core_properties.created:
                    metadata["doc_created"] = doc.core_properties.created.isoformat()
                if doc.core_properties.modified:
                    metadata["doc_modified"] = doc.core_properties.modified.isoformat()
            except Exception as e:
                logger.warning(f"DOCX ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return metadata
        
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
        return {}

def chunk_text(text: str, base_metadata: Dict) -> List[Dict]:
    """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• """
    try:
        # ë¬¸ë‹¨ë³„ ë¶„í• 
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        chunks = []
        current_chunk = []
        current_length = 0
        max_chunk_length = 1000  # ë¬¸ì ê¸°ì¤€
        
        for i, para in enumerate(paragraphs):
            if current_length + len(para) > max_chunk_length and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_text = "\n".join(current_chunk)
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    "chunk_index": len(chunks),
                    "chunk_type": "paragraph_group",
                    "word_count": len(chunk_text.split()),
                    "source_type": "document"
                })
                
                chunks.append({
                    "text": chunk_text,
                    "metadata": chunk_metadata
                })
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [para]
                current_length = len(para)
            else:
                current_chunk.append(para)
                current_length += len(para)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            chunk_text = "\n".join(current_chunk)
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update({
                "chunk_index": len(chunks),
                "chunk_type": "paragraph_group",
                "word_count": len(chunk_text.split()),
                "source_type": "document"
            })
            
            chunks.append({
                "text": chunk_text,
                "metadata": chunk_metadata
            })
        
        return chunks
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
        return []

class DocumentRAGSystem(RAGSystem):
    def __init__(self):
        super().__init__()
        self.supported_formats = ['.pdf', '.docx']
    
    def add_document(self, file_path: str, custom_metadata: Dict = None) -> bool:
        """ë¬¸ì„œ íŒŒì¼ì„ RAG ì‹œìŠ¤í…œì— ì¶”ê°€"""
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = extract_text_from_file(file_path)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            doc_metadata = extract_document_metadata(file_path)
            
            # ì‚¬ìš©ì ì •ì˜ ë©”íƒ€ë°ì´í„° ë³‘í•©
            if custom_metadata:
                doc_metadata.update(custom_metadata)
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = chunk_text(text_content, doc_metadata)
            
            if not chunks:
                logger.warning(f"ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            self.vectorstore.add_texts(
                texts=[chunk["text"] for chunk in chunks],
                metadatas=[chunk["metadata"] for chunk in chunks]
            )
            
            logger.info(f"ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {file_path}, ì²­í¬ ìˆ˜: {len(chunks)}")
            return True
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
            return False
    
    def get_document_list(self) -> List[Dict]:
        """ë“±ë¡ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        try:
            if self.vectorstore is None:
                return []
            
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê³ ìœ í•œ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            collection = self.vectorstore._collection
            results = collection.get()
            
            if not results or not results['metadatas']:
                return []
            
            # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
            documents = {}
            for i, metadata in enumerate(results['metadatas']):
                if metadata and 'file_name' in metadata:
                    file_name = metadata['file_name']
                    if file_name not in documents:
                        documents[file_name] = {
                            'file_name': file_name,
                            'title': metadata.get('title', file_name),
                            'author': metadata.get('author', 'Unknown'),
                            'upload_date': metadata.get('upload_date', metadata.get('created_date', 'Unknown')),
                            'category': metadata.get('category', 'document'),
                            'chunk_count': 0
                        }
                    documents[file_name]['chunk_count'] += 1
            
            return list(documents.values())
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

# RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¬¸ì„œ ì§€ì› í¬í•¨)
rag_system = DocumentRAGSystem()

## ---------------- 1ë‹¨ê³„ : ì‚¬ì „ì¤€ë¹„ ----------------------

# 1) State ì„ ì–¸ --------------------
class PopulationAnalysisState(TypedDict):
    # ê³ ì • ì •ë³´
    dong_name: str
    population_data: Dict[str, float]  # Double íƒ€ì… ì²˜ë¦¬
    time_stats: List[Dict]
    gender_stats: Dict[str, float]     # Double íƒ€ì… ì²˜ë¦¬
    age_stats: Dict[str, float]        # Double íƒ€ì… ì²˜ë¦¬
    data_summary: str
    analysis_strategy: Dict[str, Dict]
    
    # RAG ê´€ë ¨ ì •ë³´ ì¶”ê°€
    rag_context: List[str]
    relevant_knowledge: str

    # ë¶„ì„ ë¡œê·¸
    current_analysis: str
    current_result: str
    current_step: str
    analysis_log: List[Dict[str, str]]
    evaluation: List[Dict[str, str]]
    next_step: str
    final_report: str

# 2) RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ --------------------
def retrieve_rag_context(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ë¶„ì„ì— í•„ìš”í•œ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    dong_name = state.get("dong_name", "")
    population_data = state.get("population_data", {})
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    queries = [
        f"{dong_name} ì¸êµ¬ íŠ¹ì„±",
        f"ê°•ë‚¨êµ¬ {dong_name} ì§€ì—­ ì •ë³´",
        f"ì™¸êµ­ì¸ ë¹„ìœ¨ {population_data.get('longForeigner', 0)}% ì¸êµ¬ ë¶„ì„",
        "ì„œìš¸ì‹œ ìƒí™œì¸êµ¬ ë°ì´í„° ë¶„ì„",
        "ì™¸êµ­ì¸ê³¼ ë‚´êµ­ì¸ì˜ ìƒí™œ íŒ¨í„´ ë¶„ì„"
    ]
    
    # RAGì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    all_contexts = []
    for query in queries:
        contexts = rag_system.retrieve_relevant_context(query, k=5)
        all_contexts.extend(contexts)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_contexts = list(set(all_contexts))
    relevant_knowledge = "\n\n".join(unique_contexts[:8])  # ìƒìœ„ 8ê°œë§Œ ì‚¬ìš©
    
    state["rag_context"] = unique_contexts
    state["relevant_knowledge"] = relevant_knowledge
    
    logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ: {len(unique_contexts)}ê°œ ë¬¸ì„œ")
    return state

# 3) ë°ì´í„° ë¶„ì„ (RAG ê°•í™”) --------------------
def analyze_population_data(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ì¸êµ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)"""
    dong_name = state.get("dong_name", "")
    population_data = state.get("population_data", {})
    time_stats = state.get("time_stats", [])
    gender_stats = state.get("gender_stats", {})
    age_stats = state.get("age_stats", {})
    relevant_knowledge = state.get("relevant_knowledge", "")
    
    if not population_data:
        logger.warning("population_dataê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        population_data = {"total": 0.0, "local": 0.0, "longForeigner": 0.0, "tempForeigner": 0.0}
    
    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (Double íƒ€ì… ì²˜ë¦¬)
    total_pop = float(population_data.get('total', 0))
    local_pop = float(population_data.get('local', 0))
    long_foreigner = float(population_data.get('longForeigner', 0))
    temp_foreigner = float(population_data.get('tempForeigner', 0))
    
    logger.info(f"ë¶„ì„í•  ë°ì´í„° - ì´ì¸êµ¬: {int(total_pop):,}ëª…, ë‚´êµ­ì¸: {int(local_pop):,}ëª…")
    
    # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ë°ì´í„° ìš”ì•½ ìƒì„±
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,
        max_tokens=500
    )
    
    summary_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ ì¸êµ¬ ë°ì´í„°ì™€ ê´€ë ¨ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
    
    [ê´€ë ¨ ì§€ì‹]
    {relevant_knowledge}
    
    [ì¸êµ¬ ë°ì´í„°]
    - ë™ ì´ë¦„: {dong_name}
    - ì´ ì¸êµ¬: {total_pop:,}ëª…
    - ë‚´êµ­ì¸: {local_pop:,}ëª…
    - ì¥ê¸° ì™¸êµ­ì¸: {long_foreigner:,}ëª…
    - ë‹¨ê¸° ì™¸êµ­ì¸: {temp_foreigner:,}ëª…
    - ì™¸êµ­ì¸ ë¹„ìœ¨: {foreigner_ratio:.1f}%
    
    [ì‹œê°„ëŒ€ë³„ ë°ì´í„°]
    {time_stats_summary}
    
    [ì„±ë³„ ë°ì´í„°]
    - ë‚¨ì„±: {male_pop:,}ëª…
    - ì—¬ì„±: {female_pop:,}ëª…
    
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
    1. ì „ì²´ì ì¸ ì¸êµ¬ ê·œëª¨ì™€ êµ¬ì„±
    2. ì™¸êµ­ì¸ ë¹„ìœ¨ì˜ ì˜ë¯¸ (ê´€ë ¨ ì§€ì‹ ì°¸ê³ )
    3. ì§€ì—­ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±
    4. ì„œìš¸ì‹œ/ê°•ë‚¨êµ¬ í‰ê· ê³¼ì˜ ë¹„êµ
    
    ê°„ê²°í•˜ê³  ê°ê´€ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš” (5ì¤„ ì´ë‚´).
    """)
    
    # ë°ì´í„° ê³„ì‚°
    foreigner_ratio = ((long_foreigner + temp_foreigner) / total_pop * 100) if total_pop > 0 else 0
    male_pop = float(gender_stats.get('male', 0))
    female_pop = float(gender_stats.get('female', 0))
    
    # ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìš”ì•½
    time_stats_summary = ""
    if time_stats:
        peak_time = max(time_stats[:6], key=lambda x: float(x.get('totalPopulation', 0)))
        time_stats_summary = f"í”¼í¬ ì‹œê°„ëŒ€: {peak_time.get('timeRange', 'N/A')} ({peak_time.get('totalPopulation', 0):,}ëª…)"
    
    # ìš”ì•½ ìƒì„±
    summary_chain = summary_prompt | llm | StrOutputParser()
    
    try:
        data_summary = summary_chain.invoke({
            "relevant_knowledge": relevant_knowledge,
            "dong_name": dong_name,
            "total_pop": int(total_pop),
            "local_pop": int(local_pop),
            "long_foreigner": int(long_foreigner),
            "temp_foreigner": int(temp_foreigner),
            "foreigner_ratio": foreigner_ratio,
            "time_stats_summary": time_stats_summary,
            "male_pop": int(male_pop),
            "female_pop": int(female_pop)
        })
        
        state["data_summary"] = data_summary
        logger.info("ë°ì´í„° ìš”ì•½ ìƒì„± ì™„ë£Œ (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        state["data_summary"] = f"{dong_name}ì˜ ì´ ì¸êµ¬ëŠ” {int(total_pop):,}ëª…ì´ë©°, ì™¸êµ­ì¸ ë¹„ìœ¨ì€ {foreigner_ratio:.1f}%ì…ë‹ˆë‹¤."
    
    return state

# 4) ë¶„ì„ ì „ëµ ìˆ˜ë¦½ --------------------
def generate_analysis_strategy(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ë¶„ì„ ì „ëµ ìˆ˜ë¦½"""
    data_summary = state.get("data_summary", "")
    dong_name = state.get("dong_name", "")

    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ {dong_name}ì˜ 'ë°ì´í„° ìš”ì•½'ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¥ íš¨ê³¼ì ì¸ ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì„¸ ê°€ì§€ ë¶„ì„ ë¶€ë¬¸ë³„ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ ë°©í–¥ê³¼ í•µì‹¬ ì§ˆë¬¸ì„ ì œì‹œí•´ ì£¼ì„¸ìš”.
ëª¨ë“  ë‹µë³€ì€ **ì œê³µëœ ë°ì´í„° ìš”ì•½ ë‚´ìš©ì— ê·¼ê±°**í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

- ë°ì´í„° ìš”ì•½:
{data_summary}

ì•„ë˜ **ëª…ì‹œëœ ë”•ì…”ë„ˆë¦¬ í˜•ì‹**ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„œì‹(ì˜ˆ: JSON, ë§ˆí¬ë‹¤ìš´)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

{{
"ì¸êµ¬êµ¬ì„±ë¶„ì„": {{
"ë¶„ì„ì „ëµ": "ì´ ì¸êµ¬ ëŒ€ë¹„ ë‚´ì™¸êµ­ì¸ ë¹„ìœ¨, ë‚¨ë…€ ì„±ë¹„, ì—°ë ¹ëŒ€ë³„ ë¶„í¬ ë“± ì¸êµ¬ êµ¬ì„±ì˜ íŠ¹ì§•ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì§€ì—­ì˜ í•µì‹¬ì ì¸ ì¸êµ¬ í”„ë¡œíŒŒì¼ì„ ì •ì˜í•©ë‹ˆë‹¤.",
"í•µì‹¬ì§ˆë¬¸": [
"ë‚´êµ­ì¸ê³¼ ì™¸êµ­ì¸ ìƒí™œì¸êµ¬ì˜ ì‹œê°„ëŒ€ë³„ í™œë™ ë°˜ê²½ ì°¨ì´ëŠ” ì§€ì—­ ìƒê¶Œì˜ ì—…ì¢… êµ¬ì„±ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ê°€?",
"ë‚¨ë…€ ì„±ë¹„ì™€ ì—°ë ¹ëŒ€ë³„ ì¸êµ¬ ë¶„í¬ê°€ ì§€ì—­ ë‚´ í¸ì˜ì‹œì„¤ ë° ì„œë¹„ìŠ¤ ì—…ì¢…ì— ì–´ë–¤ ì‹œì‚¬ì ì„ ì£¼ëŠ”ê°€?"
]
}},
"ì‹œê°„íŒ¨í„´ë¶„ì„": {{
"ë¶„ì„ì „ëµ": "ìš”ì¼ë³„, ì‹œê°„ëŒ€ë³„ ì¸êµ¬ ë³€ë™ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ íŠ¹ì • ì‹œê°„ëŒ€ì— ì¸êµ¬ê°€ ê¸‰ì¦í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ì›ì¸ì„ íŒŒì•…í•˜ê³ , ì£¼ìš” í™œë™ ì‹œê°„ëŒ€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.",
"í•µì‹¬ì§ˆë¬¸": [
"íŠ¹ì • ì‹œê°„ëŒ€ì— ìœ ì…ë˜ëŠ” ìƒí™œì¸êµ¬ì˜ ì£¼ìš” í™œë™ ëª©ì (ì‡¼í•‘, ì¶œí‡´ê·¼, ì—¬ê°€ ë“±)ì€ ë¬´ì—‡ì´ë©°, ì´ëŠ” ì§€ì—­ í¸ì˜ì‹œì„¤ì˜ ìš´ì˜ ì‹œê°„ ë° ì„œë¹„ìŠ¤ êµ¬ì„±ì— ì–´ë–¤ ì‹œì‚¬ì ì„ ì£¼ëŠ”ê°€?",
"íŠ¹ì • ì‹œê°„ëŒ€ì˜ ì¸êµ¬ ê¸‰ì¦ì´ êµí†µëŸ‰, ëŒ€ì¤‘êµí†µ ì´ìš©ë¥ ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?"
]
}},
"ì§€ì—­íŠ¹ì„±ë¶„ì„": {{
"ë¶„ì„ì „ëµ": "ì£¼ë³€ ì§€ì—­ê³¼ì˜ ì¸êµ¬ ì´ë™ íŒ¨í„´, ì™¸ë¶€ ìœ ì… ìš”ì¸(ê´€ê´‘, ìƒì—…ì‹œì„¤ ë“±)ì„ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ì§€ì—­ì´ ê°€ì§„ ê³ ìœ í•œ íŠ¹ì„±(ìƒì—…ì§€, ì£¼ê±°ì§€, ë³µí•©ì§€ ë“±)ì„ ê·œëª…í•©ë‹ˆë‹¤.",
"í•µì‹¬ì§ˆë¬¸": [
"ì´ ì§€ì—­ì˜ ì£¼ìš” ì¸êµ¬ ìœ ì… ë° ìœ ì¶œ ì›ì¸ì€ ë¬´ì—‡ì´ë©°, ì´ëŠ” ì§€ì—­ ê²½ì œ í™œì„±í™”ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?",
"ì¸êµ¬ ë³€í™” íŒ¨í„´ê³¼ ì™¸ë¶€ ìœ ì… ìš”ì¸(ì˜ˆ: ëŒ€í˜• ì‡¼í•‘ëª°, ê³µì›) ê°„ì˜ ìƒê´€ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?"
]
}}
}}
"""
)

    llm = ChatOpenAI(model="gpt-4o-mini")
    formatted_prompt = prompt.format(dong_name=dong_name, data_summary=data_summary)
    response = llm.invoke(formatted_prompt)

    # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    dict_value = response.content.strip()
    if isinstance(dict_value, str):
        try:
            strategy_dict = ast.literal_eval(dict_value)
        except Exception as e:
            raise ValueError("analysis_strategyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from e

    return {
        **state,
        "analysis_strategy": strategy_dict
    }

# 5) 1ë‹¨ê³„ í•˜ë‚˜ë¡œ ë¬¶ê¸° --------------------
def preProcessing_Analysis(dong_name: str, population_data: dict, time_stats: list, gender_stats: dict, age_stats: dict) -> PopulationAnalysisState:
    """ì „ì²˜ë¦¬ ë‹¨ê³„"""
    # state ì´ˆê¸°í™”
    initial_state: PopulationAnalysisState = {
        "dong_name": dong_name,
        "population_data": population_data,
        "time_stats": time_stats,
        "gender_stats": gender_stats,
        "age_stats": age_stats,
        "data_summary": '',
        "analysis_strategy": {},
        "rag_context": [],
        "relevant_knowledge": "",

        "current_analysis": '',
        "current_result": '',
        "current_step": '',
        "analysis_log": [],
        "evaluation": [],
        "next_step": '',
        "final_report": ''
    }

    # RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    initial_state = retrieve_rag_context(initial_state)

    # ë°ì´í„° ë¶„ì„
    state = analyze_population_data(initial_state)

    # ë¶„ì„ ì „ëµ ìˆ˜ë¦½
    state = generate_analysis_strategy(state)

    # ì²«ë²ˆì§¸ ë¶„ì„ ì‹œì‘
    analysis_strategy = state["analysis_strategy"]
    first_analysis = "ì¸êµ¬êµ¬ì„±ë¶„ì„"
    strategy_text = analysis_strategy[first_analysis]["ë¶„ì„ì „ëµ"]
    core_questions = analysis_strategy[first_analysis]["í•µì‹¬ì§ˆë¬¸"]
    selected_question = random.choice(core_questions)

    return {
            **state,
            "current_analysis": selected_question,
            "current_step": first_analysis
            }

## ---------------- 2ë‹¨ê³„ : ë¶„ì„ Agent ----------------------

# 1) ë¶„ì„ ì‹¤í–‰ --------------------
def execute_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """í˜„ì¬ ë¶„ì„ ì‹¤í–‰"""
    llm = ChatOpenAI(model="gpt-4o-mini")

    current_analysis = state.get("current_analysis", "")
    current_step = state.get("current_step", "")
    analysis_strategy = state.get("analysis_strategy", "")
    data_summary = state.get("data_summary", "")
    dong_name = state.get("dong_name", "")
    relevant_knowledge = state.get("relevant_knowledge", "")

    # ë¶„ì„ ì „ëµ ì¶”ì¶œ
    strategy_block = ""
    if isinstance(analysis_strategy, dict):
        strategy_block = analysis_strategy.get(current_step, {}).get("ë¶„ì„ì „ëµ", "")
    elif isinstance(analysis_strategy, str):
        try:
            parsed = ast.literal_eval(analysis_strategy)
            strategy_block = parsed.get(current_step, {}).get("ë¶„ì„ì „ëµ", "")
        except Exception:
            strategy_block = ""

    # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ì™€ ê´€ë ¨ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

[ê´€ë ¨ ì§€ì‹ ë² ì´ìŠ¤]
{relevant_knowledge}

[ë¶„ì„ ëŒ€ìƒ ì •ë³´]
- ì§€ì—­: {dong_name}
- ë°ì´í„° ìš”ì•½: {data_summary}
- ë¶„ì„ ì „ëµ({current_step}): {strategy}
- ë¶„ì„ ì§ˆë¬¸: {analysis}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
íŠ¹íˆ ê´€ë ¨ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. ì„œìš¸ì‹œ/ê°•ë‚¨êµ¬ í‰ê· ê³¼ì˜ ë¹„êµ
2. ì§€ì—­ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±
3. ì •ì±…ì  ë§¥ë½ê³¼ì˜ ì—°ê²°
4. ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ

ë‹µë³€ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
""")

    formatted_prompt = prompt.format(
        relevant_knowledge=relevant_knowledge,
        dong_name=dong_name,
        data_summary=data_summary,
        strategy=strategy_block,
        current_step=current_step,
        analysis=current_analysis
    )

    try:
        response = llm.invoke(formatted_prompt)
        analysis_result = response.content.strip()
        
        # (1) ë¶„ì„ ë¡œê·¸ ì €ì¥ (ì§ˆë¬¸/ë‹µë³€ 1ìŒ)
        state["analysis_log"].append({"analysis": current_analysis, "result": analysis_result})
        
        logger.info(f"ë¶„ì„ ì™„ë£Œ ({current_step}): {len(analysis_result)}ì")
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        analysis_result = f"{dong_name}ì˜ {current_step} ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
        state["analysis_log"].append({"analysis": current_analysis, "result": analysis_result})

    return {
        **state,
        "current_result": analysis_result
    }

# 2) ë¶„ì„ í‰ê°€ --------------------
def evaluate_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ë¶„ì„ ê²°ê³¼ í‰ê°€"""
    llm = ChatOpenAI(model="gpt-4o-mini")

    current_analysis = state.get("current_analysis", "")
    current_result = state.get("current_result", "")
    current_step = state.get("current_step", "")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ í‰ê°€ìì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

[ë¶„ì„ ì •ë³´]
- ë¶„ì„ ë‹¨ê³„: {current_step}
- ë¶„ì„ ì§ˆë¬¸: {analysis}
- ë¶„ì„ ê²°ê³¼: {result}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ë‘ ê°€ì§€ í•­ëª©ì— ë”°ë¼ ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
- ë¶„ì„ì˜ êµ¬ì²´ì„±: ë¶„ì„ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤ì§ˆì ì¸ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€
- ì‹¤ìš©ì„±: ë¶„ì„ ê²°ê³¼ê°€ ì‹¤ì œ í™œìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ì§€

ê° í•­ëª©ì— ëŒ€í•´ 'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ìµœì¢… ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
{{
  "ë¶„ì„ì˜ êµ¬ì²´ì„±": "ìƒ",
  "ì‹¤ìš©ì„±": "ì¤‘"
}}
""")

    formatted_prompt = prompt.format(
        current_step=current_step,
        analysis=current_analysis,
        result=current_result
    )

    response = llm.invoke(formatted_prompt)
    
    # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    eval_dict = response.content.strip()
    if isinstance(eval_dict, str):
        try:
            eval_dict = ast.literal_eval(eval_dict)
        except Exception as e:
            eval_dict = {"ë¶„ì„ì˜ êµ¬ì²´ì„±": "ì¤‘", "ì‹¤ìš©ì„±": "ì¤‘"}

    # (2) í‰ê°€ ì €ì¥ (ì¸ë±ìŠ¤ í¬í•¨)
    evaluation = state.get("evaluation", [])
    eval_dict["analysis_index"] = len(state["analysis_log"]) - 1
    evaluation.append(eval_dict)

    return {
        **state,
        "evaluation": evaluation
    }

# 3) ë‹¤ìŒ ë‹¨ê³„ ê²°ì • --------------------
def decide_next_step(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ë‹¤ìŒ ë¶„ì„ ë‹¨ê³„ ê²°ì •"""
    evaluation = state.get("evaluation", [])
    analysis_log = state.get("analysis_log", [])
    current_step = state.get("current_step", "")

    # (1) ë¶„ì„ì´ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ
    if len(analysis_log) >= 3:
        next_step = "end"
    # (2) í˜„ì¬ ë‹¨ê³„ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    elif current_step == "ì¸êµ¬êµ¬ì„±ë¶„ì„":
        next_step = "ì‹œê°„íŒ¨í„´ë¶„ì„"
    elif current_step == "ì‹œê°„íŒ¨í„´ë¶„ì„":
        next_step = "ì§€ì—­íŠ¹ì„±ë¶„ì„"
    else:
        next_step = "end"

    return {
        **state,
        "next_step": next_step
    }

# 4) ë‹¤ìŒ ë¶„ì„ ìƒì„± --------------------
def generate_next_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ë‹¤ìŒ ë¶„ì„ ì§ˆë¬¸ ìƒì„±"""
    analysis_strategy = state.get("analysis_strategy", {})
    next_step = state.get("next_step", "")
    
    if next_step in analysis_strategy:
        core_questions = analysis_strategy[next_step]["í•µì‹¬ì§ˆë¬¸"]
        selected_question = random.choice(core_questions)
        
        return {
            **state,
            "current_analysis": selected_question,
            "current_step": next_step,
            "current_result": ""
        }
    
    return state

# 5) ìµœì¢… ë³´ê³ ì„œ ìƒì„± --------------------
def generate_final_report(state: PopulationAnalysisState) -> PopulationAnalysisState:
    """ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (RAG ì»¨í…ìŠ¤íŠ¸ í™œìš©)"""
    dong_name = state.get("dong_name", "")
    analysis_log = state.get("analysis_log", [])
    relevant_knowledge = state.get("relevant_knowledge", "")
    
    # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì¢…í•© ë¶„ì„
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,
        max_tokens=800
    )
    
    # ë¶„ì„ ê²°ê³¼ ìš”ì•½
    analysis_summary = ""
    for i, log in enumerate(analysis_log):
        analysis_summary += f"{i+1}. {log['analysis']}\n   â†’ {log['result']}\n\n"
    
    # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    report_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ì¸êµ¬ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ê´€ë ¨ ì§€ì‹ ë² ì´ìŠ¤]
{relevant_knowledge}

[ë¶„ì„ ëŒ€ìƒ]
- ì§€ì—­: {dong_name}

[ì„¸ë¶€ ë¶„ì„ ê²°ê³¼]
{analysis_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ğŸ™ï¸ {dong_name} ì¸êµ¬ ë°ì´í„° ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

ğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­
(ê´€ë ¨ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3-4ê°œ)

ğŸ” ì„¸ë¶€ ë¶„ì„
(ê° ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½)

ğŸ’¡ ì‹¤ìš©ì  ì œì•ˆ
(ë°ì´í„°ì™€ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì  ì œì•ˆ 2-3ê°œ)

ğŸ“ˆ ì§€ì—­ íŠ¹ì„± ìš”ì•½
(ê°•ë‚¨êµ¬ ë° ì„œìš¸ì‹œ í‰ê· ê³¼ì˜ ë¹„êµ)

â° ë¶„ì„ ì¼ì‹œ: {timestamp}
""")
    
    try:
        report_chain = report_prompt | llm | StrOutputParser()
        final_report = report_chain.invoke({
            "relevant_knowledge": relevant_knowledge,
            "dong_name": dong_name,
            "analysis_summary": analysis_summary,
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
        logger.info("RAG ê¸°ë°˜ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ë³´ê³ ì„œ ìƒì„±
        report_sections = []
        for i, log in enumerate(analysis_log):
            report_sections.append(f"""
{i+1}. {log['analysis']}
   â†’ {log['result']}
""")
        
        final_report = f"""
ğŸ™ï¸ {dong_name} ì¸êµ¬ ë°ì´í„° ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

{''.join(report_sections)}

ğŸ“Š ë¶„ì„ ì™„ë£Œ: {len(analysis_log)}ê°œ í•­ëª©
â° ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    return {
        **state,
        "final_report": final_report
    }

# 6) Agent --------------------
# ë¶„ê¸° íŒë‹¨ í•¨ìˆ˜
def route_next(state: PopulationAnalysisState) -> Literal["generate", "summarize"]:
    return "summarize" if state["next_step"] == "end" else "generate"

# ê·¸ë˜í”„ ì •ì˜ ì‹œì‘
builder = StateGraph(PopulationAnalysisState)

# ë…¸ë“œ ì¶”ê°€
builder.add_node("execute", execute_analysis)
builder.add_node("evaluate", evaluate_analysis)
builder.add_node("decide", decide_next_step)
builder.add_node("generate", generate_next_analysis)
builder.add_node("summarize", generate_final_report)

# ë…¸ë“œ ì—°ê²°
builder.set_entry_point("execute")
builder.add_edge("execute", "evaluate")
builder.add_edge("evaluate", "decide")
builder.add_conditional_edges("decide", route_next)
builder.add_edge("generate", "execute")      # ë£¨í”„
builder.add_edge("summarize", END)           # ì¢…ë£Œ

# ì»´íŒŒì¼
graph = builder.compile()

## ---------------- FastAPI ì—”ë“œí¬ì¸íŠ¸ ----------------------

@app.get("/")
async def root():
    return {"message": "ì¸êµ¬ ë°ì´í„° LangGraph Agent ë¶„ì„ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤", "status": "running"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": "gpt-4o-mini",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/analyze/langgraph")
async def analyze_with_langgraph(data: Dict[str, Any]):
    """LangGraph Agentë¥¼ ì‚¬ìš©í•œ ì¸êµ¬ ë°ì´í„° ë¶„ì„ (RAG í†µí•©)"""
    
    try:
        dong_name = data.get('dongName', 'ì•Œ ìˆ˜ ì—†ëŠ” ë™')
        population_data = data.get('populationData', {})
        time_stats = data.get('timeStats', [])
        gender_stats = data.get('genderStats', {})
        age_stats = data.get('ageStats', {})
        
        logger.info(f"{dong_name} LangGraph ë¶„ì„ ì‹œì‘ (RAG í†µí•©)...")
        logger.info(f"ë°›ì€ ì¸êµ¬ ë°ì´í„°: {population_data}")
        logger.info(f"ë°›ì€ ì‹œê°„ëŒ€ ë°ì´í„° ê°œìˆ˜: {len(time_stats) if time_stats else 0}")
        logger.info(f"ë°›ì€ ì„±ë³„ ë°ì´í„°: {gender_stats}")
        logger.info(f"ë°›ì€ ì—°ë ¹ ë°ì´í„° íƒ€ì…: {type(age_stats)}, ë‚´ìš©: {age_stats}")
        
        # ì „ì²˜ë¦¬ ë‹¨ê³„ (RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í¬í•¨)
        initial_state = preProcessing_Analysis(
            dong_name, population_data, time_stats, gender_stats, age_stats
        )
        
        # LangGraph ì‹¤í–‰
        final_state = graph.invoke(initial_state)
        
        logger.info("LangGraph ë¶„ì„ ì™„ë£Œ (RAG í†µí•©)")
        
        return {
            "status": "success",
            "dong_name": dong_name,
            "data_summary": final_state.get("data_summary", ""),
            "analysis_strategy": final_state.get("analysis_strategy", {}),
            "analysis_log": final_state.get("analysis_log", []),
            "evaluation": final_state.get("evaluation", []),
            "final_report": final_state.get("final_report", ""),
            "rag_context_count": len(final_state.get("rag_context", [])),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"LangGraph ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/rag/add-knowledge")
async def add_custom_knowledge(data: Dict[str, Any]):
    """RAG ì‹œìŠ¤í…œì— ì‚¬ìš©ì ì •ì˜ ì§€ì‹ ì¶”ê°€"""
    try:
        content = data.get('content', '')
        metadata = data.get('metadata', {})
        
        if not content:
            raise HTTPException(status_code=400, detail="ì§€ì‹ ë‚´ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # RAG ì‹œìŠ¤í…œì— ì§€ì‹ ì¶”ê°€
        rag_system.add_custom_knowledge(content, metadata)
        
        return {
            "status": "success",
            "message": "ì‚¬ìš©ì ì •ì˜ ì§€ì‹ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "content_length": len(content),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ì§€ì‹ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì§€ì‹ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/rag/status")
async def get_rag_status():
    """RAG ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    try:
        return {
            "status": "success",
            "rag_initialized": rag_system.retriever is not None,
            "vectorstore_ready": rag_system.vectorstore is not None,
            "embeddings_ready": rag_system.embeddings is not None,
            "supported_formats": rag_system.supported_formats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"RAG ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.post("/rag/upload-document")
async def upload_document(
    file: UploadFile = File(...),
    title: str = Form(None),
    category: str = Form("document"),
    description: str = Form(None)
):
    """ë¬¸ì„œ íŒŒì¼ ì—…ë¡œë“œ ë° RAG ì‹œìŠ¤í…œì— ì¶”ê°€"""
    try:
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        if not file.filename.lower().endswith(('.pdf', '.docx')):
            raise HTTPException(
                status_code=400, 
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤."
            )
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = "./temp_documents"
        os.makedirs(temp_dir, exist_ok=True)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file_path = os.path.join(temp_dir, file.filename)
        
        with open(temp_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # ì‚¬ìš©ì ì •ì˜ ë©”íƒ€ë°ì´í„°
        custom_metadata = {
            "title": title or file.filename,
            "category": category,
            "description": description,
            "upload_date": datetime.now().isoformat()
        }
        
        # RAG ì‹œìŠ¤í…œì— ì¶”ê°€
        success = rag_system.add_document(temp_file_path, custom_metadata)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(temp_file_path)
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        if success:
            return {
                "status": "success",
                "message": "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "filename": file.filename,
                "metadata": custom_metadata,
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/rag/documents")
async def get_documents():
    """ë“±ë¡ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        documents = rag_system.get_document_list()
        return {
            "status": "success",
            "documents": documents,
            "count": len(documents),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/rag/documents/{filename}")
async def delete_document(filename: str):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë“¤ ì‚­ì œ
        if rag_system.vectorstore is None:
            raise HTTPException(status_code=404, detail="RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        collection = rag_system.vectorstore._collection
        results = collection.get()
        
        if not results or not results['metadatas']:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•´ë‹¹ íŒŒì¼ì˜ ì²­í¬ë“¤ ì°¾ê¸°
        delete_ids = []
        for i, metadata in enumerate(results['metadatas']):
            if metadata and metadata.get('file_name') == filename:
                delete_ids.append(results['ids'][i])
        
        if delete_ids:
            collection.delete(ids=delete_ids)
            logger.info(f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {filename}, ì‚­ì œëœ ì²­í¬ ìˆ˜: {len(delete_ids)}")
            return {
                "status": "success",
                "message": f"ë¬¸ì„œ '{filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "deleted_chunks": len(delete_ids),
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    
    logger.info("=" * 60)
    logger.info("ğŸš€ ì¸êµ¬ ë°ì´í„° LangGraph Agent with RAG & Document Support ì„œë²„ ì‹œì‘")
    logger.info(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://{HOST}:{PORT}")
    logger.info(f"ğŸ”§ í™˜ê²½: {ENVIRONMENT}")
    logger.info(f"ğŸ§  ëª¨ë¸: GPT-4o-mini")
    logger.info(f"ğŸ“š RAG: ChromaDB + Sentence Transformers")
    logger.info(f"ğŸ“„ ë¬¸ì„œ ì§€ì›: PDF, DOCX")
    logger.info("=" * 60)
    
    try:
        uvicorn.run(
            app,
            host=HOST,
            port=PORT,
            log_level="info",
            reload=False
        )
    except KeyboardInterrupt:
        logger.info("ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        logger.error(f"ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
