# ####### ì¸êµ¬ ë°ì´í„° ë¶„ì„ìš© LangGraph Agent #######
# ## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ---------------------------------------------
# import pandas as pd
# import numpy as np
# import os
# import openai
# import random
# import ast
# from fastapi import FastAPI, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from datetime import datetime
# from typing import List, Dict, Any, Literal, TypedDict
# import json
# import logging
# import warnings
# from dotenv import load_dotenv

# # .env íŒŒì¼ ë¡œë“œ
# load_dotenv()

# warnings.filterwarnings("ignore", category=DeprecationWarning)

# from typing import Annotated, Literal, Sequence, TypedDict, List, Dict
# from langchain import hub
# from langchain_core.messages import BaseMessage, HumanMessage
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
# from langchain_openai import ChatOpenAI
# from langchain_community.embeddings import OpenAIEmbeddings
# from langchain_community.vectorstores import Chroma
# from langchain.output_parsers import CommaSeparatedListOutputParser
# from langgraph.graph import StateGraph, END

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# HOST = os.getenv("HOST", "0.0.0.0")
# PORT = 8004  # Population LangGraph ì „ìš© í¬íŠ¸
# ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

# # API í‚¤ í™•ì¸
# if not OPENAI_API_KEY:
#     logger.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
#     raise ValueError("OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

# logger.info(f"ì„œë²„ í™˜ê²½: {ENVIRONMENT}")
# logger.info(f"OpenAI API í‚¤ ì„¤ì •ë¨: {'*' * 10}{OPENAI_API_KEY[-4:] if OPENAI_API_KEY else 'None'}")

# app = FastAPI(title="ì¸êµ¬ ë°ì´í„° LangGraph Agent ë¶„ì„ API", version="1.0.0")

# # CORS ì„¤ì •
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["http://localhost:3000"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# ## ---------------- 1ë‹¨ê³„ : ì‚¬ì „ì¤€ë¹„ ----------------------

# # 1) State ì„ ì–¸ --------------------
# class PopulationAnalysisState(TypedDict):
#     # ê³ ì • ì •ë³´
#     dong_name: str
#     population_data: Dict[str, float]  # Double íƒ€ì… ì²˜ë¦¬
#     time_stats: List[Dict]
#     gender_stats: Dict[str, float]     # Double íƒ€ì… ì²˜ë¦¬
#     age_stats: Dict[str, float]        # Double íƒ€ì… ì²˜ë¦¬
#     data_summary: str
#     analysis_strategy: Dict[str, Dict]

#     # ë¶„ì„ ë¡œê·¸
#     current_analysis: str
#     current_result: str
#     current_step: str
#     analysis_log: List[Dict[str, str]]
#     evaluation: List[Dict[str, str]]
#     next_step: str
#     final_report: str

# # 2) ë°ì´í„° ë¶„ì„ --------------------
# def analyze_population_data(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ì¸êµ¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½ ìƒì„±"""
#     dong_name = state.get("dong_name", "")
#     population_data = state.get("population_data", {})
#     time_stats = state.get("time_stats", [])
#     gender_stats = state.get("gender_stats", {})
#     age_stats = state.get("age_stats", {})
    
#     if not population_data:
#         logger.warning("population_dataê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
#         population_data = {"total": 0.0, "local": 0.0, "longForeigner": 0.0, "tempForeigner": 0.0}
    
#     # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (Double íƒ€ì… ì²˜ë¦¬)
#     total_pop = float(population_data.get('total', 0))
#     local_pop = float(population_data.get('local', 0))
#     logger.info(f"ë¶„ì„í•  ë°ì´í„° - ì´ì¸êµ¬: {int(total_pop):,}ëª…, ë‚´êµ­ì¸: {int(local_pop):,}ëª…")

#     llm = ChatOpenAI(model="gpt-4o-mini")

#     # ìš”ì•½ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     summary_prompt = ChatPromptTemplate.from_template(
#         '''ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” AIì…ë‹ˆë‹¤.
#         ë‹¤ìŒ {dong_name}ì˜ ì¸êµ¬ ë°ì´í„°ì—ì„œ ë¶„ì„ì„ ìœ„í•œ ì¤‘ìš”í•œ ë‚´ìš©ì„ 5ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

#         ì¸êµ¬ ë°ì´í„°:
#         - ì´ ì¸êµ¬: {total}ëª…
#         - ë‚´êµ­ì¸: {local}ëª…  
#         - ì¥ê¸° ì™¸êµ­ì¸: {long_foreigner}ëª…
#         - ë‹¨ê¸° ì™¸êµ­ì¸: {temp_foreigner}ëª…
#         - ë‚¨ì„±: {male}ëª…
#         - ì—¬ì„±: {female}ëª…

#         ì‹œê°„ëŒ€ë³„ íŒ¨í„´: {time_pattern}
#         '''
#     )
    
#     # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ìš”ì•½
#     time_pattern = "ë°ì´í„° ì—†ìŒ"
#     if time_stats and len(time_stats) > 0:
#         # ì˜¬ë°”ë¥¸ í‚¤ ì‚¬ìš©: totalPopulation, timeRange (Double íƒ€ì… ì²˜ë¦¬)
#         peak_time = max(time_stats, key=lambda x: float(x.get('totalPopulation', 0)))
#         low_time = min(time_stats, key=lambda x: float(x.get('totalPopulation', 0)))
#         peak_pop = int(float(peak_time.get('totalPopulation', 0)))
#         low_pop = int(float(low_time.get('totalPopulation', 0)))
#         time_pattern = f"í”¼í¬: {peak_time.get('timeRange', 'N/A')}ì‹œ ({peak_pop:,}ëª…), ìµœì €: {low_time.get('timeRange', 'N/A')}ì‹œ ({low_pop:,}ëª…)"
    
#     formatted_summary_prompt = summary_prompt.format(
#         dong_name=dong_name,
#         total=int(float(population_data.get('total', 0))),
#         local=int(float(population_data.get('local', 0))),
#         long_foreigner=int(float(population_data.get('longForeigner', 0))),  # Double íƒ€ì… ì²˜ë¦¬
#         temp_foreigner=int(float(population_data.get('tempForeigner', 0))),  # Double íƒ€ì… ì²˜ë¦¬
#         male=int(float(gender_stats.get('male', 0))),
#         female=int(float(gender_stats.get('female', 0))),
#         time_pattern=time_pattern
#     )
    
#     summary_response = llm.invoke(formatted_summary_prompt)
#     data_summary = summary_response.content.strip()

#     return {
#         **state,
#         "data_summary": data_summary,
#     }

# # 3) ë¶„ì„ ì „ëµ ìˆ˜ë¦½ --------------------
# def generate_analysis_strategy(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ë¶„ì„ ì „ëµ ìˆ˜ë¦½"""
#     data_summary = state.get("data_summary", "")
#     dong_name = state.get("dong_name", "")

#     prompt = ChatPromptTemplate.from_template("""
# ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ {dong_name}ì˜ 'ë°ì´í„° ìš”ì•½'ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¥ íš¨ê³¼ì ì¸ ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
# ë‹¤ìŒ ì„¸ ê°€ì§€ ë¶„ì„ ë¶€ë¬¸ë³„ë¡œ ì‹¬ì¸µì ì¸ ë¶„ì„ ë°©í–¥ê³¼ í•µì‹¬ ì§ˆë¬¸ì„ ì œì‹œí•´ ì£¼ì„¸ìš”.
# ëª¨ë“  ë‹µë³€ì€ **ì œê³µëœ ë°ì´í„° ìš”ì•½ ë‚´ìš©ì— ê·¼ê±°**í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

# - ë°ì´í„° ìš”ì•½:
# {data_summary}

# ì•„ë˜ **ëª…ì‹œëœ ë”•ì…”ë„ˆë¦¬ í˜•ì‹**ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ì¶œë ¥í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„œì‹(ì˜ˆ: JSON, ë§ˆí¬ë‹¤ìš´)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

# {{
# "ì¸êµ¬êµ¬ì„±ë¶„ì„": {{
# "ë¶„ì„ì „ëµ": "ì´ ì¸êµ¬ ëŒ€ë¹„ ë‚´ì™¸êµ­ì¸ ë¹„ìœ¨, ë‚¨ë…€ ì„±ë¹„, ì—°ë ¹ëŒ€ë³„ ë¶„í¬ ë“± ì¸êµ¬ êµ¬ì„±ì˜ íŠ¹ì§•ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì§€ì—­ì˜ í•µì‹¬ì ì¸ ì¸êµ¬ í”„ë¡œíŒŒì¼ì„ ì •ì˜í•©ë‹ˆë‹¤.",
# "í•µì‹¬ì§ˆë¬¸": [
# "ë‚´êµ­ì¸ê³¼ ì™¸êµ­ì¸ ìƒí™œì¸êµ¬ì˜ ì‹œê°„ëŒ€ë³„ í™œë™ ë°˜ê²½ ì°¨ì´ëŠ” ì§€ì—­ ìƒê¶Œì˜ ì—…ì¢… êµ¬ì„±ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ”ê°€?",
# "ë‚¨ë…€ ì„±ë¹„ì™€ ì—°ë ¹ëŒ€ë³„ ì¸êµ¬ ë¶„í¬ê°€ ì§€ì—­ ë‚´ í¸ì˜ì‹œì„¤ ë° ì„œë¹„ìŠ¤ ì—…ì¢…ì— ì–´ë–¤ ì‹œì‚¬ì ì„ ì£¼ëŠ”ê°€?"
# ]
# }},
# "ì‹œê°„íŒ¨í„´ë¶„ì„": {{
# "ë¶„ì„ì „ëµ": "ìš”ì¼ë³„, ì‹œê°„ëŒ€ë³„ ì¸êµ¬ ë³€ë™ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ íŠ¹ì • ì‹œê°„ëŒ€ì— ì¸êµ¬ê°€ ê¸‰ì¦í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ì›ì¸ì„ íŒŒì•…í•˜ê³ , ì£¼ìš” í™œë™ ì‹œê°„ëŒ€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.",
# "í•µì‹¬ì§ˆë¬¸": [
# "íŠ¹ì • ì‹œê°„ëŒ€ì— ìœ ì…ë˜ëŠ” ìƒí™œì¸êµ¬ì˜ ì£¼ìš” í™œë™ ëª©ì (ì‡¼í•‘, ì¶œí‡´ê·¼, ì—¬ê°€ ë“±)ì€ ë¬´ì—‡ì´ë©°, ì´ëŠ” ì§€ì—­ í¸ì˜ì‹œì„¤ì˜ ìš´ì˜ ì‹œê°„ ë° ì„œë¹„ìŠ¤ êµ¬ì„±ì— ì–´ë–¤ ì‹œì‚¬ì ì„ ì£¼ëŠ”ê°€?",
# "íŠ¹ì • ì‹œê°„ëŒ€ì˜ ì¸êµ¬ ê¸‰ì¦ì´ êµí†µëŸ‰, ëŒ€ì¤‘êµí†µ ì´ìš©ë¥ ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?"
# ]
# }},
# "ì§€ì—­íŠ¹ì„±ë¶„ì„": {{
# "ë¶„ì„ì „ëµ": "ì£¼ë³€ ì§€ì—­ê³¼ì˜ ì¸êµ¬ ì´ë™ íŒ¨í„´, ì™¸ë¶€ ìœ ì… ìš”ì¸(ê´€ê´‘, ìƒì—…ì‹œì„¤ ë“±)ì„ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ì§€ì—­ì´ ê°€ì§„ ê³ ìœ í•œ íŠ¹ì„±(ìƒì—…ì§€, ì£¼ê±°ì§€, ë³µí•©ì§€ ë“±)ì„ ê·œëª…í•©ë‹ˆë‹¤.",
# "í•µì‹¬ì§ˆë¬¸": [
# "ì´ ì§€ì—­ì˜ ì£¼ìš” ì¸êµ¬ ìœ ì… ë° ìœ ì¶œ ì›ì¸ì€ ë¬´ì—‡ì´ë©°, ì´ëŠ” ì§€ì—­ ê²½ì œ í™œì„±í™”ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?",
# "ì¸êµ¬ ë³€í™” íŒ¨í„´ê³¼ ì™¸ë¶€ ìœ ì… ìš”ì¸(ì˜ˆ: ëŒ€í˜• ì‡¼í•‘ëª°, ê³µì›) ê°„ì˜ ìƒê´€ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?"
# ]
# }}
# }}
# """
# )

#     llm = ChatOpenAI(model="gpt-4o-mini")
#     formatted_prompt = prompt.format(dong_name=dong_name, data_summary=data_summary)
#     response = llm.invoke(formatted_prompt)

#     # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
#     dict_value = response.content.strip()
#     if isinstance(dict_value, str):
#         try:
#             strategy_dict = ast.literal_eval(dict_value)
#         except Exception as e:
#             raise ValueError("analysis_strategyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from e

#     return {
#         **state,
#         "analysis_strategy": strategy_dict
#     }

# # 4) 1ë‹¨ê³„ í•˜ë‚˜ë¡œ ë¬¶ê¸° --------------------
# def preProcessing_Analysis(dong_name: str, population_data: dict, time_stats: list, gender_stats: dict, age_stats: dict) -> PopulationAnalysisState:
#     """ì „ì²˜ë¦¬ ë‹¨ê³„"""
#     # state ì´ˆê¸°í™”
#     initial_state: PopulationAnalysisState = {
#         "dong_name": dong_name,
#         "population_data": population_data,
#         "time_stats": time_stats,
#         "gender_stats": gender_stats,
#         "age_stats": age_stats,
#         "data_summary": '',
#         "analysis_strategy": {},

#         "current_analysis": '',
#         "current_result": '',
#         "current_step": '',
#         "analysis_log": [],
#         "evaluation": [],
#         "next_step": '',
#         "final_report": ''
#     }

#     # ë°ì´í„° ë¶„ì„
#     state = analyze_population_data(initial_state)

#     # ë¶„ì„ ì „ëµ ìˆ˜ë¦½
#     state = generate_analysis_strategy(state)

#     # ì²«ë²ˆì§¸ ë¶„ì„ ì‹œì‘
#     analysis_strategy = state["analysis_strategy"]
#     first_analysis = "ì¸êµ¬êµ¬ì„±ë¶„ì„"
#     strategy_text = analysis_strategy[first_analysis]["ë¶„ì„ì „ëµ"]
#     core_questions = analysis_strategy[first_analysis]["í•µì‹¬ì§ˆë¬¸"]
#     selected_question = random.choice(core_questions)

#     return {
#             **state,
#             "current_analysis": selected_question,
#             "current_step": first_analysis
#             }

# ## ---------------- 2ë‹¨ê³„ : ë¶„ì„ Agent ----------------------

# # 1) ë¶„ì„ ì‹¤í–‰ --------------------
# def execute_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """í˜„ì¬ ë¶„ì„ ì‹¤í–‰"""
#     llm = ChatOpenAI(model="gpt-4o-mini")

#     current_analysis = state.get("current_analysis", "")
#     current_step = state.get("current_step", "")
#     analysis_strategy = state.get("analysis_strategy", "")
#     data_summary = state.get("data_summary", "")
#     dong_name = state.get("dong_name", "")

#     # ë¶„ì„ ì „ëµ ì¶”ì¶œ
#     strategy_block = ""
#     if isinstance(analysis_strategy, dict):
#         strategy_block = analysis_strategy.get(current_step, {}).get("ë¶„ì„ì „ëµ", "")
#     elif isinstance(analysis_strategy, str):
#         try:
#             parsed = ast.literal_eval(analysis_strategy)
#             strategy_block = parsed.get(current_step, {}).get("ë¶„ì„ì „ëµ", "")
#         except Exception:
#             strategy_block = ""

#     # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     prompt = ChatPromptTemplate.from_template("""
# ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
# ë‹¤ìŒì˜ ì°¸ì¡° ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

# [ì°¸ê³  ì •ë³´]
# - ì§€ì—­: {dong_name}
# - ë°ì´í„° ìš”ì•½: {data_summary}
# - ë¶„ì„ ì „ëµ({current_step}): {strategy}
# - ë¶„ì„ ì§ˆë¬¸: {analysis}

# ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
# ë‹µë³€ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
# """)

#     formatted_prompt = prompt.format(
#         dong_name=dong_name,
#         data_summary=data_summary,
#         strategy=strategy_block,
#         current_step=current_step,
#         analysis=current_analysis
#     )

#     response = llm.invoke(formatted_prompt)
#     analysis_result = response.content.strip()

#     # (1) ë¶„ì„ ë¡œê·¸ ì €ì¥ (ì§ˆë¬¸/ë‹µë³€ 1ìŒ)
#     state["analysis_log"].append({"analysis": current_analysis, "result": analysis_result})

#     return {
#         **state,
#         "current_result": analysis_result
#     }

# # 2) ë¶„ì„ í‰ê°€ --------------------
# def evaluate_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ë¶„ì„ ê²°ê³¼ í‰ê°€"""
#     llm = ChatOpenAI(model="gpt-4o-mini")

#     current_analysis = state.get("current_analysis", "")
#     current_result = state.get("current_result", "")
#     current_step = state.get("current_step", "")

#     # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     prompt = ChatPromptTemplate.from_template("""
# ë‹¹ì‹ ì€ ì¸êµ¬ ë°ì´í„° ë¶„ì„ í‰ê°€ìì…ë‹ˆë‹¤.
# ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

# [ë¶„ì„ ì •ë³´]
# - ë¶„ì„ ë‹¨ê³„: {current_step}
# - ë¶„ì„ ì§ˆë¬¸: {analysis}
# - ë¶„ì„ ê²°ê³¼: {result}

# ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ ë‘ ê°€ì§€ í•­ëª©ì— ë”°ë¼ ë¶„ì„ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
# - ë¶„ì„ì˜ êµ¬ì²´ì„±: ë¶„ì„ì´ ì–¼ë§ˆë‚˜ êµ¬ì²´ì ì´ê³  ì‹¤ì§ˆì ì¸ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€
# - ì‹¤ìš©ì„±: ë¶„ì„ ê²°ê³¼ê°€ ì‹¤ì œ í™œìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ì§€

# ê° í•­ëª©ì— ëŒ€í•´ 'ìƒ', 'ì¤‘', 'í•˜' ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

# ìµœì¢… ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
# {{
#   "ë¶„ì„ì˜ êµ¬ì²´ì„±": "ìƒ",
#   "ì‹¤ìš©ì„±": "ì¤‘"
# }}
# """)

#     formatted_prompt = prompt.format(
#         current_step=current_step,
#         analysis=current_analysis,
#         result=current_result
#     )

#     response = llm.invoke(formatted_prompt)
    
#     # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
#     eval_dict = response.content.strip()
#     if isinstance(eval_dict, str):
#         try:
#             eval_dict = ast.literal_eval(eval_dict)
#         except Exception as e:
#             eval_dict = {"ë¶„ì„ì˜ êµ¬ì²´ì„±": "ì¤‘", "ì‹¤ìš©ì„±": "ì¤‘"}

#     # (2) í‰ê°€ ì €ì¥ (ì¸ë±ìŠ¤ í¬í•¨)
#     evaluation = state.get("evaluation", [])
#     eval_dict["analysis_index"] = len(state["analysis_log"]) - 1
#     evaluation.append(eval_dict)

#     return {
#         **state,
#         "evaluation": evaluation
#     }

# # 3) ë‹¤ìŒ ë‹¨ê³„ ê²°ì • --------------------
# def decide_next_step(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ë‹¤ìŒ ë¶„ì„ ë‹¨ê³„ ê²°ì •"""
#     evaluation = state.get("evaluation", [])
#     analysis_log = state.get("analysis_log", [])
#     current_step = state.get("current_step", "")

#     # (1) ë¶„ì„ì´ 3íšŒë¥¼ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ
#     if len(analysis_log) >= 3:
#         next_step = "end"
#     # (2) í˜„ì¬ ë‹¨ê³„ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
#     elif current_step == "ì¸êµ¬êµ¬ì„±ë¶„ì„":
#         next_step = "ì‹œê°„íŒ¨í„´ë¶„ì„"
#     elif current_step == "ì‹œê°„íŒ¨í„´ë¶„ì„":
#         next_step = "ì§€ì—­íŠ¹ì„±ë¶„ì„"
#     else:
#         next_step = "end"

#     return {
#         **state,
#         "next_step": next_step
#     }

# # 4) ë‹¤ìŒ ë¶„ì„ ìƒì„± --------------------
# def generate_next_analysis(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ë‹¤ìŒ ë¶„ì„ ì§ˆë¬¸ ìƒì„±"""
#     analysis_strategy = state.get("analysis_strategy", {})
#     next_step = state.get("next_step", "")
    
#     if next_step in analysis_strategy:
#         core_questions = analysis_strategy[next_step]["í•µì‹¬ì§ˆë¬¸"]
#         selected_question = random.choice(core_questions)
        
#         return {
#             **state,
#             "current_analysis": selected_question,
#             "current_step": next_step,
#             "current_result": ""
#         }
    
#     return state

# # 5) ìµœì¢… ë³´ê³ ì„œ ìƒì„± --------------------
# def generate_final_report(state: PopulationAnalysisState) -> PopulationAnalysisState:
#     """ìµœì¢… ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
#     dong_name = state.get("dong_name", "")
#     analysis_log = state.get("analysis_log", [])
    
#     report_sections = []
#     for i, log in enumerate(analysis_log):
#         report_sections.append(f"""
# {i+1}. {log['analysis']}
#    â†’ {log['result']}
# """)
    
#     final_report = f"""
# ğŸ™ï¸ {dong_name} ì¸êµ¬ ë°ì´í„° ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

# {''.join(report_sections)}

# ğŸ“Š ë¶„ì„ ì™„ë£Œ: {len(analysis_log)}ê°œ í•­ëª©
# â° ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# """
    
#     return {
#         **state,
#         "final_report": final_report
#     }

# # 6) Agent --------------------
# # ë¶„ê¸° íŒë‹¨ í•¨ìˆ˜
# def route_next(state: PopulationAnalysisState) -> Literal["generate", "summarize"]:
#     return "summarize" if state["next_step"] == "end" else "generate"

# # ê·¸ë˜í”„ ì •ì˜ ì‹œì‘
# builder = StateGraph(PopulationAnalysisState)

# # ë…¸ë“œ ì¶”ê°€
# builder.add_node("execute", execute_analysis)
# builder.add_node("evaluate", evaluate_analysis)
# builder.add_node("decide", decide_next_step)
# builder.add_node("generate", generate_next_analysis)
# builder.add_node("summarize", generate_final_report)

# # ë…¸ë“œ ì—°ê²°
# builder.set_entry_point("execute")
# builder.add_edge("execute", "evaluate")
# builder.add_edge("evaluate", "decide")
# builder.add_conditional_edges("decide", route_next)
# builder.add_edge("generate", "execute")      # ë£¨í”„
# builder.add_edge("summarize", END)           # ì¢…ë£Œ

# # ì»´íŒŒì¼
# graph = builder.compile()

# ## ---------------- FastAPI ì—”ë“œí¬ì¸íŠ¸ ----------------------

# @app.get("/")
# async def root():
#     return {"message": "ì¸êµ¬ ë°ì´í„° LangGraph Agent ë¶„ì„ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤", "status": "running"}

# @app.get("/health")
# async def health_check():
#     return {
#         "status": "healthy",
#         "model": "gpt-4o-mini",
#         "timestamp": datetime.now().isoformat()
#     }

# @app.post("/analyze/langgraph")
# async def analyze_with_langgraph(data: Dict[str, Any]):
#     """LangGraph Agentë¥¼ ì‚¬ìš©í•œ ì¸êµ¬ ë°ì´í„° ë¶„ì„"""
    
#     try:
#         dong_name = data.get('dongName', 'ì•Œ ìˆ˜ ì—†ëŠ” ë™')
#         population_data = data.get('populationData', {})
#         time_stats = data.get('timeStats', [])
#         gender_stats = data.get('genderStats', {})
#         age_stats = data.get('ageStats', {})
        
#         logger.info(f"{dong_name} LangGraph ë¶„ì„ ì‹œì‘...")
#         logger.info(f"ë°›ì€ ì¸êµ¬ ë°ì´í„°: {population_data}")
#         logger.info(f"ë°›ì€ ì‹œê°„ëŒ€ ë°ì´í„° ê°œìˆ˜: {len(time_stats) if time_stats else 0}")
#         logger.info(f"ë°›ì€ ì„±ë³„ ë°ì´í„°: {gender_stats}")
#         logger.info(f"ë°›ì€ ì—°ë ¹ ë°ì´í„° íƒ€ì…: {type(age_stats)}, ë‚´ìš©: {age_stats}")
        
#         # ì „ì²˜ë¦¬ ë‹¨ê³„
#         initial_state = preProcessing_Analysis(
#             dong_name, population_data, time_stats, gender_stats, age_stats
#         )
        
#         # LangGraph ì‹¤í–‰
#         final_state = graph.invoke(initial_state)
        
#         logger.info("LangGraph ë¶„ì„ ì™„ë£Œ")
        
#         return {
#             "status": "success",
#             "dong_name": dong_name,
#             "data_summary": final_state.get("data_summary", ""),
#             "analysis_strategy": final_state.get("analysis_strategy", {}),
#             "analysis_log": final_state.get("analysis_log", []),
#             "evaluation": final_state.get("evaluation", []),
#             "final_report": final_state.get("final_report", ""),
#             "timestamp": datetime.now().isoformat()
#         }
        
#     except Exception as e:
#         logger.error(f"LangGraph ë¶„ì„ ì‹¤íŒ¨: {e}")
#         raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# if __name__ == "__main__":
#     import uvicorn
#     logger.info(f"ì„œë²„ ì‹œì‘: {HOST}:{PORT}")
#     uvicorn.run(app, host=HOST, port=PORT)
